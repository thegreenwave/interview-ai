import os
import random
import difflib
import numpy as np
import librosa
import streamlit as st
from openai import OpenAI
import plotly.graph_objects as go
import plotly.express as px
import pdfplumber
from pdf2image import convert_from_bytes
import pytesseract
from collections import Counter
import json

# ==========================================
# ğŸ”‘ ê¸°ë³¸ ì„¤ì •
# ==========================================
st.set_page_config(page_title="Spec-trum Pro", page_icon="ğŸ™ï¸", layout="wide")

password = st.text_input("ğŸ”’ ì ‘ì† ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
if password != "0601": 
    st.warning("ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")
    st.stop()

if "OPENAI_API_KEY" in st.secrets:
    os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
else:
    pass

client = OpenAI()

# ==========================================
# ğŸ“Š ë¶„ì„ í•¨ìˆ˜ ëª¨ìŒ
# ==========================================
def extract_text_from_pdf(pdf_file):
    """
    í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì¶œ ë°©ì‹:
    1. í…ìŠ¤íŠ¸ ë ˆì´ì–´ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ì½ìŒ (ë¹ ë¦„)
    2. ì—†ìœ¼ë©´(ì´ë¯¸ì§€ë©´) OCRë¡œ ê°•ì œë¡œ ì½ìŒ (ëŠë¦¼ but í™•ì‹¤)
    """
    text = ""
    
    # 1. ë¨¼ì € ì¼ë°˜ì ì¸ ë°©ì‹ìœ¼ë¡œ ì‹œë„ (pdfplumber)
    try:
        with pdfplumber.open(pdf_file) as pdf:
            for page in pdf.pages:
                extracted = page.extract_text()
                if extracted:
                    text += extracted + "\n"
    except:
        pass 

    # 2. í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ìœ¼ë©´(50ì ë¯¸ë§Œ) ì´ë¯¸ì§€ë¡œ ê°„ì£¼í•˜ê³  OCR ìˆ˜í–‰
    if len(text) < 50:
        st.toast("âš ï¸ ì´ë¯¸ì§€ PDFê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. OCR ë³€í™˜ì„ ì‹œë„í•©ë‹ˆë‹¤. (ì‹œê°„ì´ ì¢€ ê±¸ë ¤ìš”!)")
        
        # íŒŒì¼ í¬ì¸í„° ì´ˆê¸°í™”
        pdf_file.seek(0)
        
        # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ ë‚´ ì²˜ë¦¬)
        try:
            images = convert_from_bytes(pdf_file.read())
            text = ""
            # ê° ì´ë¯¸ì§€ë¥¼ ìˆœíšŒí•˜ë©° í•œê¸€ ì¶”ì¶œ
            for image in images:
                page_text = pytesseract.image_to_string(image, lang='kor+eng')
                text += page_text + "\n"
        except Exception as e:
            st.error(f"OCR ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""
            
    return text

def analyze_audio_features(y, sr):
    rms = librosa.feature.rms(y=y)[0]
    times = librosa.times_like(rms, sr=sr)
    cent = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
    non_silent_intervals = librosa.effects.split(y, top_db=25)
    non_silent_duration = sum([(end - start) for start, end in non_silent_intervals]) / sr
    total_duration = librosa.get_duration(y=y, sr=sr)
    
    if len(non_silent_intervals) > 0:
        initial_silence = librosa.frames_to_time(non_silent_intervals[0][0], sr=sr)
    else:
        initial_silence = 0
    
    silence_duration = total_duration - non_silent_duration
    silence_ratio = silence_duration / total_duration if total_duration > 0 else 0
    
    return times, rms, cent, total_duration, non_silent_duration, silence_duration, silence_ratio, initial_silence

def analyze_text_patterns(text):
    fillers = ["ìŒ", "ì–´", "ê·¸", "ë§‰", "ì´ì œ", "ì•½ê°„", "ì €", "ì‚¬ì‹¤"]
    filler_counts = {f: text.count(f) for f in fillers if text.count(f) > 0}
    total_fillers = sum(filler_counts.values())
    words = text.replace(".", "").split()
    valid_words = [w for w in words if len(w) >= 2 and w not in fillers]
    top_keywords = Counter(valid_words).most_common(5)
    return filler_counts, total_fillers, top_keywords

def calculate_similarity(text1, text2):
    matcher = difflib.SequenceMatcher(None, text1, text2)
    return matcher.ratio() * 100

# ==========================================
# ğŸ›ï¸ ë©”ì¸ UI
# ==========================================
with st.sidebar:
    st.title("SPEC-TRUM")
    st.caption("OCR ì—”ì§„ íƒ‘ì¬ ë²„ì „")
    st.markdown("---")
    menu = st.radio("ê¸°ëŠ¥ ì„ íƒ", ["1. ë°œí‘œ ì¤€ë¹„", "2. ìƒê¸°ë¶€ ì‹¬ì¸µ ë©´ì ‘"])

# ==========================================
# [ê¸°ëŠ¥ 1] ë°œí‘œ ì¤€ë¹„
# ==========================================
if menu == "1. ë°œí‘œ ì¤€ë¹„":
    st.title("ğŸ¤ ë°œí‘œ ì¤€ë¹„")
    tab1, tab2, tab3 = st.tabs(["ğŸ“ ëŒ€ë³¸ ì‘ì„±", "ğŸ§ ëŒ€ë³¸ í‰ê°€", "ğŸ“Š ë°œí‘œ ëŠ¥ë ¥ í‰ê°€"])
    
    with tab1:
        st.header("ëŒ€ë³¸ ì‘ì„±")
        col1, col2 = st.columns(2)
        with col1:
            p_topic = st.text_input("ì£¼ì œ", placeholder="ì£¼ì œ ì…ë ¥")
            p_context = st.text_input("ìƒí™©", placeholder="ìƒí™© ì…ë ¥")
        with col2:
            p_req = st.text_area("ìš”êµ¬ì‚¬í•­", placeholder="ìš”êµ¬ì‚¬í•­ ì…ë ¥")
            
        if st.button("ëŒ€ë³¸ ìƒì„±"):
            if p_topic:
                with st.spinner("ì‘ì„± ì¤‘..."):
                    prompt = f"ì£¼ì œ:{p_topic}\nìƒí™©:{p_context}\nìš”êµ¬ì‚¬í•­:{p_req}\në°œí‘œëŒ€ë³¸ ì‘ì„±í•´ì¤˜."
                    res = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role":"user", "content":prompt}])
                    st.session_state['script'] = res.choices[0].message.content
                    st.success("ì™„ë£Œ!")

        if 'script' in st.session_state:
            st.text_area("ìƒì„±ëœ ëŒ€ë³¸", st.session_state['script'])

    with tab2:
        st.header("ëŒ€ë³¸ í‰ê°€")
        u_script = st.text_area("ëŒ€ë³¸ ì…ë ¥", value=st.session_state.get('script', ""))
        u_intent = st.text_input("ì˜ë„")
        if st.button("í‰ê°€ ë°›ê¸°") and u_script:
            with st.spinner("í‰ê°€ ì¤‘..."):
                res = client.chat.completions.create(model="gpt-4o", messages=[{"role":"user", "content":f"ëŒ€ë³¸:{u_script}\nì˜ë„:{u_intent}\ní‰ê°€í•´ì¤˜."}])
                st.info(res.choices[0].message.content)

    with tab3:
        st.header("ë°œí‘œ ëŠ¥ë ¥ í‰ê°€")
        ref_text = st.text_area("ê¸°ì¤€ ëŒ€ë³¸", value=st.session_state.get('script', ""), height=100)
        audio_input = st.audio_input("ë…¹ìŒ ì‹œì‘")
        if audio_input and ref_text:
            with st.spinner("ë¶„ì„ ì¤‘..."):
                y, sr = librosa.load(audio_input, sr=None)
                times, rms, cent, total_dur, _, _, _, _ = analyze_audio_features(y, sr)
                tempo = float(librosa.beat.beat_track(y=y, sr=sr)[0])
                audio_input.seek(0)
                transcript = client.audio.transcriptions.create(model="whisper-1", file=audio_input).text
                acc = calculate_similarity(ref_text, transcript)
                
                c1, c2, c3 = st.columns(3)
                c1.metric("ì†ë„", f"{tempo:.0f} BPM")
                c2.metric("ì •í™•ë„", f"{acc:.1f}%")
                c3.metric("ì‹œê°„", f"{total_dur:.1f}ì´ˆ")
                
                fig = go.Figure()
                fig.add_trace(go.Scatter(x=times, y=rms, fill='tozeroy', name='Volume'))
                st.plotly_chart(fig, use_container_width=True)

# ==========================================
# [ê¸°ëŠ¥ 2] ìƒê¸°ë¶€ ì‹¬ì¸µ ë©´ì ‘ (OCR ì ìš©)
# ==========================================
elif menu == "2. ìƒê¸°ë¶€ ì‹¬ì¸µ ë©´ì ‘":
    st.title("ğŸ“ ìƒí™œê¸°ë¡ë¶€ ê¸°ë°˜ ë©´ì ‘")
    st.markdown("ì´ë¯¸ì§€ë¡œ ëœ PDFë„ ì½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (OCR ì—”ì§„ ê°€ë™)")
    
    uploaded_file = st.file_uploader("ìƒê¸°ë¶€ PDF ì—…ë¡œë“œ", type="pdf")
    
    if uploaded_file:
        with st.spinner("ìƒê¸°ë¶€ë¥¼ ì½ê³  ìˆìŠµë‹ˆë‹¤... (ì´ë¯¸ì§€ì¼ ê²½ìš° 1~2ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
            text = extract_text_from_pdf(uploaded_file)
            
            if len(text) > 50:
                st.success(f"âœ… ë¶„ì„ ì™„ë£Œ! (ì´ {len(text)}ì ì½ìŒ)")
                if st.button("ì§ˆë¬¸ ìƒì„±í•˜ê¸°"):
                    prompt = f"ìƒê¸°ë¶€ ë‚´ìš©:\n{text[:15000]}\nì „ê³µì í•©ì„±/ì¸ì„± ë©´ì ‘ ì§ˆë¬¸ 3ê°œ ë§Œë“¤ì–´ì¤˜."
                    res = client.chat.completions.create(model="gpt-4o-mini", messages=[{"role":"user", "content":prompt}])
                    st.session_state['uni_questions'] = res.choices[0].message.content
            else:
                st.error("ì´ë¯¸ì§€ ë³€í™˜ì— ì‹¤íŒ¨í–ˆê±°ë‚˜ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")

    if 'uni_questions' in st.session_state:
        st.info(st.session_state['uni_questions'])
        target_q = st.text_input("ì§ˆë¬¸ ì…ë ¥")
        audio_input = st.audio_input("ë…¹ìŒ")
        if audio_input and target_q:
            with st.spinner("í‰ê°€ ì¤‘..."):
                audio_input.seek(0)
                transcript = client.audio.transcriptions.create(model="whisper-1", file=audio_input).text
                
                eval_prompt = f"ì§ˆë¬¸:{target_q}\në‹µë³€:{transcript}\ní‰ê°€í•­ëª©: ë…¼ë¦¬ì„±, ì§„ì •ì„±, ìì‹ ê°, ì í•©ì„±. JSONìœ¼ë¡œ ì ìˆ˜ì™€ í”¼ë“œë°± ì¤˜."
                res = client.chat.completions.create(model="gpt-4o", messages=[{"role":"user", "content":eval_prompt}], response_format={"type":"json_object"})
                eval_data = json.loads(res.choices[0].message.content)
                
                st.write(eval_data.get('feedback'))
                
                # ë ˆì´ë” ì°¨íŠ¸
                categories = ['ë…¼ë¦¬ì„±', 'ì§„ì •ì„±', 'ìì‹ ê°', 'ì í•©ì„±']
                scores = [eval_data.get('logic',0)*10, eval_data.get('sincerity',0)*10, eval_data.get('confidence',0)*10, eval_data.get('suitability',0)*10]
                fig = go.Figure(data=go.Scatterpolar(r=scores, theta=categories, fill='toself'))
                fig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0, 100])), showlegend=False)
                st.plotly_chart(fig)
